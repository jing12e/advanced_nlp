{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925d7490-eceb-47ec-afd4-058185dd5907",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T08:02:57.628848300Z",
     "start_time": "2024-03-02T08:02:52.567609900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  路路路路路路路路\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\katko\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4185369c2aeddffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T08:03:09.167477300Z",
     "start_time": "2024-03-02T08:03:08.756115500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"token_classification_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e66291332607cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T08:03:19.819027300Z",
     "start_time": "2024-03-02T08:03:19.802028300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc992a4-f301-4e59-9ba4-fabca8814a4f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3acb54-8a43-45a3-8993-c784282d739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import extract_data_from_conll_extended\n",
    "extract_data_from_conll_extended(\"data/en_ewt-up-test.conllu\", \"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f57dffa9a221e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T08:03:50.718490300Z",
     "start_time": "2024-03-02T08:03:49.564125200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8db9fd99b9a450f86ae1ab06b02132a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9880e904be44f92a37516e33e4fb0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['arguments', 'predicate', 'word', 'predicate_position'],\n",
       "        num_rows: 40482\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['arguments', 'predicate', 'word', 'predicate_position'],\n",
       "        num_rows: 4799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "datasets = load_dataset('json', data_files={'train': 'train.json', 'test': 'test.json'})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1314a051618b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T08:04:06.072630300Z",
     "start_time": "2024-03-02T08:04:06.055630900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arguments': ['_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'ARG0',\n",
       "  'V',\n",
       "  'ARG1',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'ARGM-LOC',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " 'predicate': 'kill.01',\n",
       " 'word': ['Al',\n",
       "  '-',\n",
       "  'Zaman',\n",
       "  ':',\n",
       "  'American',\n",
       "  'forces',\n",
       "  'killed',\n",
       "  'Shaikh',\n",
       "  'Abdullah',\n",
       "  'al',\n",
       "  '-',\n",
       "  'Ani',\n",
       "  ',',\n",
       "  'the',\n",
       "  'preacher',\n",
       "  'at',\n",
       "  'the',\n",
       "  'mosque',\n",
       "  'in',\n",
       "  'the',\n",
       "  'town',\n",
       "  'of',\n",
       "  'Qaim',\n",
       "  ',',\n",
       "  'near',\n",
       "  'the',\n",
       "  'Syrian',\n",
       "  'border',\n",
       "  '.'],\n",
       " 'predicate_position': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6fae141fbc0385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T08:04:23.990387200Z",
     "start_time": "2024-03-02T08:04:23.975385900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6b7ec65470697f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T08:04:29.769044100Z",
     "start_time": "2024-03-02T08:04:29.715044800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arguments</th>\n",
       "      <th>predicate</th>\n",
       "      <th>word</th>\n",
       "      <th>predicate_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, V, _, _, ARGM-ADJ, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]</td>\n",
       "      <td>solve.01</td>\n",
       "      <td>[The, U.S., State, Department, and, the, misnamed, think, tanks, that, follow, its, lead, have, held, numerous, conferences, on, Kashmir, and, ,, in, most, of, them, ,, the, solution, that, has, emerged, is, a, valley, prized, loose, from, Indian, control, and, under, its, own, version, of, Ibrahim, Rugova, .]</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ARGM-ADJ, V, _]</td>\n",
       "      <td>serve.02</td>\n",
       "      <td>[Great, service, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, ARG0, _, _, _, V, _, _, _, ARG1, _, _, _]</td>\n",
       "      <td>know.01</td>\n",
       "      <td>[I, 'm, getting, a, 10, gallon, for, my, betta, tomorrow, and, I, just, want, to, know, how, to, properly, set, it, up, .]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[_, _, _, _, _, _, V, _, _, ARG1, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]</td>\n",
       "      <td>notice.03</td>\n",
       "      <td>[They, are, deemed, to, be, on, notice, that, they, had, alternate, point, rights, (, it, s, in, the, tariff, ), and, if, they, wanted, to, submit, a, bid, that, had, a, different, rate, for, primaries, and, alternates, ,, they, could, have, done, so, .]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[_, _, ARG1, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, V, _, _, _, ARG2, _]</td>\n",
       "      <td>fall.04</td>\n",
       "      <td>[A, third, report, ,, \", The, Impacts, of, Climate, Change, :, An, Appraisal, for, the, Future, ,, \", completed, by, the, Britain, -, based, International, Policy, Network, and, released, almost, simultaneously, with, the, other, two, ,, falls, somewhat, into, that, category, .]</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ARG0, V, ARG1, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]</td>\n",
       "      <td>make.05</td>\n",
       "      <td>[I, MAKE, MONEY, NOT, DEAL, WITH, BROKERS, \", Wow, ,, can, you, believe, in, today, s, tough, times, this, dealership, would, be, looking, for, any, way, to, move, vehicles, .]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, ARG2, ARGM-MOD, _, ARG1, _, V, _, _]</td>\n",
       "      <td>send.01</td>\n",
       "      <td>[Is, there, an, article, to, be, included, in, an, Enron, publication, and, in, addition, is, a, letter, to, be, sent, under, John, 's, name, (, if, so, ,, to, whom, will, the, letter, be, sent, ), ?]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[V, _, ARGM-EXT, _]</td>\n",
       "      <td>thank.01</td>\n",
       "      <td>[Thanks, a, lot, .]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[_, _, _, _, _, _, _, _, _, _, _, _, _, _, ARG0, _, V, _, _, _, _, _, _, _]</td>\n",
       "      <td>open_up.03</td>\n",
       "      <td>[Now, that, Sibley, is, not, a, candidate, for, Lt., Governor, ,, there, is, more, freedom, to, open, up, and, ask, some, difficult, questions, .]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[_, _, _, _, _, _, V, _, ARG1, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _]</td>\n",
       "      <td>way.01</td>\n",
       "      <td>[I, was, not, happy, with, the, way, they, looked, ,, very, wavy, ,, uneven, edges, ,, and, with, the, exception, of, 1, ,, there, is, a, dip, in, the, center, of, each, nail, .]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4e8cefa-fb3f-4334-8222-94b1e9641f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arguments': ['_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'ARG0',\n",
       "  'V',\n",
       "  'ARG1',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  'ARGM-LOC',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_',\n",
       "  '_'],\n",
       " 'predicate': 'kill.01',\n",
       " 'word': ['Al',\n",
       "  '-',\n",
       "  'Zaman',\n",
       "  ':',\n",
       "  'American',\n",
       "  'forces',\n",
       "  'killed',\n",
       "  'Shaikh',\n",
       "  'Abdullah',\n",
       "  'al',\n",
       "  '-',\n",
       "  'Ani',\n",
       "  ',',\n",
       "  'the',\n",
       "  'preacher',\n",
       "  'at',\n",
       "  'the',\n",
       "  'mosque',\n",
       "  'in',\n",
       "  'the',\n",
       "  'town',\n",
       "  'of',\n",
       "  'Qaim',\n",
       "  ',',\n",
       "  'near',\n",
       "  'the',\n",
       "  'Syrian',\n",
       "  'border',\n",
       "  '.'],\n",
       " 'predicate_position': 7}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4a00a-883c-4620-b88d-fc3345d33e2d",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237f4338dcca93b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T09:09:22.530273900Z",
     "start_time": "2024-03-02T09:09:22.181391900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2996c7fe621ae6a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T09:09:23.431553200Z",
     "start_time": "2024-03-02T09:09:23.411302100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4858eeaa61a663d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T09:09:24.745351400Z",
     "start_time": "2024-03-02T09:09:24.731350300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(instance):\n",
    "    def align(tokenized_input, labels):\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        aligned_labels = ['_'] * len(word_ids) \n",
    "        label_index = 0 \n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            try:\n",
    "                if word_id is None:\n",
    "                    aligned_labels[i] = '[PAD]'\n",
    "                    continue \n",
    "                original_label = labels[word_id]\n",
    "                if original_label == '_':\n",
    "                    continue \n",
    "                if i == 0 or word_id != word_ids[i-1]:\n",
    "                    prefix = ''\n",
    "                else:\n",
    "                    prefix = ''\n",
    "                aligned_labels[i] = f'{prefix}{original_label}'\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        return aligned_labels\n",
    "    sentence = instance[\"word\"]\n",
    "    labels = instance[\"arguments\"]\n",
    "    tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "    predicate = instance['predicate']\n",
    "    predicate = tokenizer(predicate)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "    aligned_labels = align(tokenized_input, labels)\n",
    "    tokenized_input['input_ids'].extend(predicate['input_ids'][1:]), tokenized_input['attention_mask'].extend(predicate['attention_mask'][1:]), aligned_labels.extend(['[PAD]' for i in range(len(predicate['input_ids'][1:]))])\n",
    "    tokenized_input['labels'] = aligned_labels\n",
    "    \n",
    "    return tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92718ab9-217a-4d7a-b9c8-b91613a25c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40482\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets['train']))\n",
    "training_set = [tokenize(instance) for instance in datasets['train']]\n",
    "test_set = [tokenize(instance) for instance in datasets['test']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e57dd6-e735-48a0-a182-37051b60fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2632, 1011, 23564, 2386, 1024, 2137, 2749, 2730, 21146, 28209, 14093, 2632, 1011, 2019, 2072, 1010, 1996, 14512, 2012, 1996, 8806, 1999, 1996, 2237, 1997, 1053, 4886, 2213, 1010, 2379, 1996, 9042, 3675, 1012, 102, 3102, 1012, 5890, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': ['[PAD]', '_', '_', '_', '_', '_', '_', 'ARG0', 'V', 'ARG1', 'ARG1', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'ARGM-LOC', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']}\n"
     ]
    }
   ],
   "source": [
    "sample = training_set[0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "287db1bd-6289-4b69-a9d4-1747102c5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Sequence, Value\n",
    "\n",
    "features = Features({\n",
    "    \"input_ids\": Sequence(feature=Value(dtype='int64')),\n",
    "    \"attention_mask\": Sequence(feature=Value(dtype='int64')),\n",
    "    \"labels\": Sequence(feature=Value(dtype='int64')),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e11cd7-3183-4cc0-8fe6-bbff43de4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = ['R-ARGM-ADJ', '_', 'ARGM-ADJ']\n",
    "label_list = list(set(label for instance in training_set for label in instance['labels'] + unseen))\n",
    "label_dict = {label:int(i) for i, label in enumerate(list(label_list))}\n",
    "label_dict['[PAD]'] = -100\n",
    "for instance in training_set:\n",
    "    instance['labels'] = [int(label_dict[label]) for label in instance['labels']]\n",
    "for instance in test_set:\n",
    "    instance['labels'] = [int(label_dict[label]) for label in instance['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7f3c443-2b4c-4c1c-8201-69468605b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_dict({\"input_ids\": [item['input_ids'] for item in training_set],\n",
    "                             \"attention_mask\": [item['attention_mask'] for item in training_set],\n",
    "                             \"labels\": [item['labels'] for item in training_set]}, features=features)\n",
    "\n",
    "dataset_test = Dataset.from_dict({\"input_ids\": [item['input_ids'] for item in test_set],\n",
    "                             \"attention_mask\": [item['attention_mask'] for item in test_set],\n",
    "                             \"labels\": [item['labels'] for item in test_set]}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94a7b34f43933b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-02T09:09:26.160324200Z",
     "start_time": "2024-03-02T09:09:26.148324700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2632, 1011, 23564, 2386, 1024, 2137, 2749, 2730, 21146, 28209, 14093, 2632, 1011, 2019, 2072, 1010, 1996, 14512, 2012, 1996, 8806, 1999, 1996, 2237, 1997, 1053, 4886, 2213, 1010, 2379, 1996, 9042, 3675, 1012, 102, 3102, 1012, 5890, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 32, 32, 32, 32, 32, 32, 24, 34, 2, 2, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 7, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "#print(tokenized_input.word_ids())\n",
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e12cd73680f6939f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f010cfb-0ee7-4ba4-882e-5d83fe291505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2632,\n",
       "  1011,\n",
       "  23564,\n",
       "  2386,\n",
       "  1024,\n",
       "  2137,\n",
       "  2749,\n",
       "  2730,\n",
       "  21146,\n",
       "  28209,\n",
       "  14093,\n",
       "  2632,\n",
       "  1011,\n",
       "  2019,\n",
       "  2072,\n",
       "  1010,\n",
       "  1996,\n",
       "  14512,\n",
       "  2012,\n",
       "  1996,\n",
       "  8806,\n",
       "  1999,\n",
       "  1996,\n",
       "  2237,\n",
       "  1997,\n",
       "  1053,\n",
       "  4886,\n",
       "  2213,\n",
       "  1010,\n",
       "  2379,\n",
       "  1996,\n",
       "  9042,\n",
       "  3675,\n",
       "  1012,\n",
       "  102,\n",
       "  3102,\n",
       "  1012,\n",
       "  5890,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  24,\n",
       "  34,\n",
       "  2,\n",
       "  2,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  7,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358ca3f-63b2-4df9-83da-c7ca2964c75e",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d5094dc-1fb6-4706-8451-0c03449c7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68f52781-dbe4-4c67-bda0-5b641950faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27138d5c-1d91-4084-8675-b1f949b18e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katko\\AppData\\Local\\Temp\\ipykernel_25504\\2951164050.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b9e9f9-0c5c-48b8-98c1-eaf836b71064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARG2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-GOL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARG2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-PRR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-GOL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG4 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-DIR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-COM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARG4 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-CXN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARG3 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-TMP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-EXT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-COM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGA seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-PRP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-DIR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-TMP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-CAU seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-MNR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARG3 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG5 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-ADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: _ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARG1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: V seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-MOD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-DIS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARG0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-PRR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-ADV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARG0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-MNR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-CAU seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-PRD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARG1-DSP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARG4 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-PRP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-CXN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-NEG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-ADJ seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-LVB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-MNR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-EXT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-COM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-DIR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARGM-REC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG1-DSP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: C-ARGM-GOL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [PAD] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ARG3 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: R-ARGM-TMP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADJ': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ADV': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARG0': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARG1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARG1-DSP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARG2': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARG3': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARG4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARGM-ADJ': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARGM-ADV': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARGM-CAU': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARGM-COM': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARGM-CXN': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARGM-DIR': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARGM-EXT': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARGM-GOL': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARGM-LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARGM-MNR': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ARGM-PRP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARGM-PRR': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ARGM-TMP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'CAU': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'COM': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'CXN': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'DIR': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'DIS': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'DSP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'EXT': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'GOL': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'LVB': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'MNR': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'MOD': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'NEG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PAD]': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PRD': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PRP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PRR': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'REC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'RG0': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'RG1': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'RG2': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'RG3': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'RG4': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'RG5': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'RGA': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'TMP': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'V': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " '_': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(label_list)\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e7af19e-b45d-4a9a-a650-1c0d8afee453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1665595e-bd25-47b2-90a3-be56619e46f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6ca14-aed3-41cd-b002-a0d71a09972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_model(pickles):\n",
    "    '''\n",
    "    Input: string name with .pickle extension. Must be present in working directory. Check with 'pwd' command\n",
    "    Loads pretrained model variable from pickle file at filepath(pickle).\n",
    "    '''\n",
    "    with open(pickles, 'rb') as p:\n",
    "        return pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295719d-f3b9-4534-94e8-e5d873fdfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('basic_bert.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fac29-8efb-44dc-b30d-fe835f003289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2d7de-5e13-4521-b519-56392cb81c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a16040-a79c-4746-b79b-8b6ea6599087",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset_test.select_columns(['input_ids','attention_mask']), dataset_test.select_columns(['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f83ee8-6e2e-4506-91bf-77293f950e57",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519ed02-45b5-4d16-9b5d-712817ba394c",
   "metadata": {},
   "source": [
    "#### This part is unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef0311-7415-4a16-a8c8-f94e160fac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = trainer.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f044c7-333d-441e-9471-86b52be055a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fe140-9d21-465d-b411-05bbfd6475c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639a357-16d0-4b1e-a624-fe3dae5e0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.predictions[0][47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeac5fe-9010-47fb-b2bd-9dd91fa899bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(outputs.predictions, axis=2)\n",
    "\n",
    "predictions = predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13dd6c-3e5a-4f75-85d9-6fbe7c73ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame()\n",
    "df_eval['predictions'] = predictions\n",
    "df_eval['true_labels'] = y\n",
    "df_eval.to_csv('outputs/test_set_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdb83d-b063-4bfb-b945-2dbcca2666cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def picklify(pickles,not_so_pickles):\n",
    "    '''\n",
    "    Saves a model or variable to a pickle file \n",
    "    '''\n",
    "    with open(pickles, 'wb') as p:\n",
    "        pickle.dump(not_so_pickles, p)\n",
    "picklify('basic_bert.pickle',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a6f2d-c0f3-4eac-b547-430275782cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707aeaed-79d6-4258-9b2e-9d854b3e0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36dac2d9-b687-4ceb-8f50-03c27751d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,  df_test = pd.DataFrame(dataset_train), pd.DataFrame(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eff570-aaa2-4309-8fdb-c849b6650259",
   "metadata": {},
   "source": [
    "### Replace -100 labels with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "618cc842-33dc-4065-b5c3-8fbe01a7b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['labels'], df_train['labels'] = df_test['labels'].apply(lambda x: [e if e != -100 else 61 for e in x]), df_train['labels'].apply(lambda x: [e if e != -100 else 0 for e in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7ddd3-8fd8-478c-a7cc-09d0d0bcc492",
   "metadata": {},
   "source": [
    "## Create generator for training advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a6df8f8-94b3-457e-85fb-7d0df9c70e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_generator(df, folder_path, chunk_size):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    chunk_start = 0\n",
    "    chunk_end = chunk_size\n",
    "    chunk_count = len(df) // chunk_size + (1 if len(df) % chunk_size else 0)\n",
    "    for i in range(chunk_count):\n",
    "        chunk = df[chunk_start:chunk_end]\n",
    "        file_name = f\"chunk_{i}.csv\"\n",
    "        chunk.to_csv(os.path.join(folder_path, file_name), index=False)\n",
    "        chunk_start += chunk_size\n",
    "        chunk_end += chunk_size\n",
    "\n",
    "create_generator(df_train, 'df_train_folder', chunk_size=50)\n",
    "create_generator(df_test, 'df_test_folder', chunk_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a46641b-3d2a-4732-b96c-2ea93ede2f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['input_ids'].map(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258a2a9-34fd-4479-a535-431e9149d4b8",
   "metadata": {},
   "source": [
    "#### ^Due to this length we will set our padding length max_len to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58ed1f52-5795-4ef5-976f-b3453779bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525978d5-a094-4731-8b67-89a7a04dcec7",
   "metadata": {},
   "source": [
    "## Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c0a7dbb-8113-4939-bd28-56d3a751c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(folder_path, max_len=200):\n",
    "    def pad(tokens):\n",
    "        tokens = tokens.replace('[', '').replace(']', '')\n",
    "        tokens = tf.convert_to_tensor([int(token) for token in tokens.split(',')], dtype=tf.int32)\n",
    "        if tf.size(tokens) > max_len:\n",
    "            return tokens[tf.size(tokens)-max_len-1:]\n",
    "        else:\n",
    "            padding = tf.constant([-100 for _ in range(max_len - tf.size(tokens))], dtype=tf.int32)\n",
    "            return tf.concat([tokens, padding], axis=0)\n",
    "    \n",
    "    def extend(attention):\n",
    "        attention = [int(x) for x in attention.replace('[', '').replace(']', '').split(',')]\n",
    "        return tf.convert_to_tensor(attention + [0] * (max_len - len(attention)), dtype=tf.int32)\n",
    "\n",
    "    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path)]\n",
    "    random.shuffle(file_paths)\n",
    "    for file_path in file_paths:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            df = pd.read_csv(file_path)\n",
    "            # print(len(df))\n",
    "            for _, row in df.iterrows():\n",
    "                input_ids = pad(row['input_ids'])\n",
    "                attention_mask = extend(row['attention_mask'])\n",
    "                batch_x.append( tf.stack([input_ids, attention_mask], axis=1))\n",
    "                batch_y.append(pad(row['labels']))\n",
    "                # print( tf.stack([input_ids, attention_mask], axis=1))\n",
    "        yield tf.convert_to_tensor(batch_x), tf.convert_to_tensor(batch_y)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0495951-bdc7-4178-b569-b7d7583a432f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(50, 200, 2), dtype=int32, numpy=\n",
       " array([[[ 101,    1],\n",
       "         [1045,    1],\n",
       "         [2079,    1],\n",
       "         ...,\n",
       "         [-100,    0],\n",
       "         [-100,    0],\n",
       "         [-100,    0]],\n",
       " \n",
       "        [[ 101,    1],\n",
       "         [1045,    1],\n",
       "         [2079,    1],\n",
       "         ...,\n",
       "         [-100,    0],\n",
       "         [-100,    0],\n",
       "         [-100,    0]],\n",
       " \n",
       "        [[ 101,    1],\n",
       "         [1045,    1],\n",
       "         [2079,    1],\n",
       "         ...,\n",
       "         [-100,    0],\n",
       "         [-100,    0],\n",
       "         [-100,    0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 101,    1],\n",
       "         [2245,    1],\n",
       "         [2008,    1],\n",
       "         ...,\n",
       "         [-100,    0],\n",
       "         [-100,    0],\n",
       "         [-100,    0]],\n",
       " \n",
       "        [[ 101,    1],\n",
       "         [2092,    1],\n",
       "         [1010,    1],\n",
       "         ...,\n",
       "         [-100,    0],\n",
       "         [-100,    0],\n",
       "         [-100,    0]],\n",
       " \n",
       "        [[ 101,    1],\n",
       "         [2031,    1],\n",
       "         [2017,    1],\n",
       "         ...,\n",
       "         [-100,    0],\n",
       "         [-100,    0],\n",
       "         [-100,    0]]])>,\n",
       " <tf.Tensor: shape=(50, 200), dtype=int32, numpy=\n",
       " array([[  61,   32,   32, ..., -100, -100, -100],\n",
       "        [  61,   32,   32, ..., -100, -100, -100],\n",
       "        [  61,   32,   32, ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [  61,   32,   32, ..., -100, -100, -100],\n",
       "        [  61,   36,   32, ..., -100, -100, -100],\n",
       "        [  61,   34,   32, ..., -100, -100, -100]])>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [i for i in data_generator('df_test_folder')][0]\n",
    "input_dim = sample[0].shape\n",
    "num_labels = sample[1].shape[1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d93a33c-4e70-4e66-9b01-10502e12c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [tensor[1] for tensor in data_generator('df_train_folder') if tf.reduce_any(tensor[1] > 61)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cb6bd18-42aa-4126-8424-e240225ad6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d09e120e-f904-4c13-bc77-2d12fdd020b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 200])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ee60711-7ef7-440b-882f-8b9804f3a4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(set(label for sequence in df_train['labels'] for label in sequence))\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336afed-d85b-419b-a52c-a1dd6fa2680c",
   "metadata": {},
   "source": [
    "## Advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "370c61d0-476a-424f-b80e-69365ef9e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 2)]          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 200, 4)            80        \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 200, 24000)        1152480000\n",
      " onal)                                                           \n",
      "                                                                 \n",
      " crf (CRF)                   ((None, 200),             576000000 \n",
      "                              (None, 200, 24000),                \n",
      "                              (None,),                           \n",
      "                              (24000, 24000))                    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1728480080 (-1676014272.00 Byte)\n",
      "Trainable params: 1728480080 (-1676014272.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\layer_utils.py:146: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  total_memory_size += weight_shape * per_param_size\n",
      "C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\layer_utils.py:146: RuntimeWarning: overflow encountered in scalar add\n",
      "  total_memory_size += weight_shape * per_param_size\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional\n",
    "from tf2crf import CRF, ModelWithCRFLoss\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_lstm_crf(seq_len, feature_dim, num_labels):\n",
    "    print(f'seq_len:{seq_len}, feature_dim:{feature_dim}, num_labels:{num_labels}')\n",
    "    input = Input(shape=(seq_len, feature_dim), dtype='float32')\n",
    "    \n",
    "    bilstm_layer1 = Bidirectional(LSTM(units=feature_dim, return_sequences=True))\n",
    "    bilstm_output1 = bilstm_layer1(input)\n",
    "    \n",
    "    bilstm_layer2 = Bidirectional(LSTM(units=num_labels, return_sequences=True))\n",
    "    bilstm_output2 = bilstm_layer2(bilstm_output1)\n",
    "\n",
    "\n",
    "    \n",
    "    crf = CRF(dtype='float32')\n",
    "    output = crf(bilstm_output2)\n",
    "    \n",
    "    base_model = Model(input, output)\n",
    "    model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
    "\n",
    "    return base_model\n",
    "\n",
    "seq_len = sample[1].shape[1]\n",
    "feature_dim = sample[0].shape[2]\n",
    "num_labels = num_labels * seq_len #label for every element in the sequence\n",
    "\n",
    "model = create_lstm_crf(seq_len, feature_dim, num_labels)\n",
    "model.build(input_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62c04035-bf33-498d-9b28-a500eecce567",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(0.001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3aa11603-c57f-4caf-83fa-9213e5364375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node mean_squared_error_2/remove_squeezable_dimensions/Squeeze defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\katko\\AppData\\Local\\Temp\\ipykernel_25504\\2776165477.py\", line 1, in <module>\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 263, in call\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 200, in squeeze_or_expand_dimensions\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 139, in remove_squeezable_dimensions\n\nCan not squeeze dim[1], expected a dimension of 1, got 200\n\t [[{{node mean_squared_error_2/remove_squeezable_dimensions/Squeeze}}]] [Op:__inference_train_function_1015789]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdf_test_folder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node mean_squared_error_2/remove_squeezable_dimensions/Squeeze defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\katko\\AppData\\Local\\Temp\\ipykernel_25504\\2776165477.py\", line 1, in <module>\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 263, in call\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 200, in squeeze_or_expand_dimensions\n\n  File \"C:\\Users\\katko\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\losses_utils.py\", line 139, in remove_squeezable_dimensions\n\nCan not squeeze dim[1], expected a dimension of 1, got 200\n\t [[{{node mean_squared_error_2/remove_squeezable_dimensions/Squeeze}}]] [Op:__inference_train_function_1015789]"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_generator('df_test_folder'), batch_size=50, epochs=1, steps_per_epoch=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a44b4ca-917f-4adc-b3a9-bd2a8444748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(label for sequence in df_test['labels'] for label in sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a0e6a-4295-48c2-9674-37ce62d54279",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensors = [i[0] for i in data_generator('df_test_folder')]\n",
    "y_tensors = [i[1] for i in data_generator('df_test_folder')]\n",
    "\n",
    "if len(x_tensors[-1]) != 50:\n",
    "    x_tensors.pop() \n",
    "    y_tensors.pop()\n",
    "\n",
    "x_tensors_filtered = [tensor for tensor in x_tensors if tensor.shape[0] == 50]\n",
    "y_tensors_filtered = [tensor for tensor in y_tensors if tensor.shape[0] == 50]\n",
    "\n",
    "x = tf.convert_to_tensor(x_tensors_filtered)\n",
    "y = tf.convert_to_tensor(y_tensors_filtered)\n",
    "x = tf.reshape(x, [-1, 200, 2])\n",
    "y = tf.reshape(y, [-1, 200])\n",
    "history = model.fit(x,y, batch_size=50, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0807c-b6d7-4d4c-8755-9a4269e8b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.reshape(y, [-1, 200]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762b178-df91-4d36-9d4b-78d74939270c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
